{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegrattarola/ml-18-19/blob/master/02_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nu4HJeN686y2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "\n",
        "Prof. Cesare Alippi  \n",
        "Daniele Grattarola  ([`daniele.grattarola@usi.ch`](mailto:daniele.grattarola@usi.ch)  )    \n",
        "Daniele Zambon ([`daniele.zambon@usi.ch`](mailto:daniele.zambon@usi.ch)  )\n",
        "\n",
        "---\n",
        "# Lab 02: Classification and neural networks\n",
        "\n",
        "We have a $d$-dimensional input vector $X \\in \\mathbb{R}^d$ and a set of $k$ possible classes, $C_1, \\dots, C_k$.\n",
        "Our goal in classification is to assign $X$ to the correct class. \n",
        "\n",
        "In particular, our goal is to determine a __discriminant__ function to parition the input space.\n",
        "\n",
        "![alt text](http://atriplex.info/blog/wp-content/uploads/2017/05/th_lda.png)\n",
        "\n",
        "---\n",
        "# Simple approach: linear regression\n",
        "\n",
        "We can use the tools we already have to build a simple classifier. \n",
        "\n",
        "Given a problem with two classes $C_0, C_1$ we assign:\n",
        "\n",
        "$$\n",
        "y =\n",
        "\\begin{cases}\n",
        "    0 \\textrm{, if } X \\textrm{ is of class } C_0 \\\\\n",
        "    1 \\textrm{, if } X \\textrm{ is of class } C_1 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "and we build a dataset of observations $\\{(X_1, y_1), \\dots, (X_n, y_n)\\}$.\n",
        "\n",
        "We then solve our linear model $y = X^T \\theta$ as:\n",
        "\n",
        "$$\n",
        "\\hat\\theta = (X^T X)^{-1} X^T Y\n",
        "$$\n",
        "\n",
        "and predict new samples as:\n",
        "\n",
        "$$\n",
        "\\hat y = \n",
        "\\begin{cases}\n",
        "0 \\textrm{, if } X^T \\hat\\theta < 0.5 \\\\\n",
        "1 \\textrm{, if } X^T \\hat\\theta \\ge 0.5\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "We call __decision boundary__ the point that partitions the input space where $X^T \\hat\\theta = 0.5$.\n",
        "\n",
        "Let's see this in code..."
      ]
    },
    {
      "metadata": {
        "id": "-SjvNW28BxGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import some libraries and functions\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a classification problem\n",
        "x, y = make_classification(n_features=1,           # Dimension of the input x\n",
        "                           n_informative=1,        # Number of features that actually have meaning\n",
        "                           n_clusters_per_class=1, # Number of clusters in a class\n",
        "                           n_redundant=0,          # Number of features that carry the same information \n",
        "                           class_sep=2)            # Separation coefficient of the two classes\n",
        "\n",
        "# Let's look at the data\n",
        "plt.scatter(x, np.zeros_like(x))\n",
        "plt.xlabel('X')\n",
        "plt.yticks([]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ayq8nv8xojjG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's see the data as we would in a regression problem\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oK27_CTE9Ex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now estimate the regression parameters from the data\n",
        "X = np.hstack((x, np.ones((x.shape[0], 1))))  # Add dummy column for bias\n",
        "S = np.dot(X.T, X)\n",
        "S_inv = np.linalg.inv(S) \n",
        "theta_hat = S_inv.dot(X.T).dot(y)\n",
        "\n",
        "print('Estimated theta: {}'.format(theta_hat))\n",
        "\n",
        "# Plot the estimated linear model\n",
        "y_hat = X.dot(theta_hat)\n",
        "plt.plot(X[:, 0], y_hat)\n",
        "plt.scatter(X[:, 0], y)\n",
        "plt.ylabel('y')\n",
        "plt.xlabel('X');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BNVqEpJOgBq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Are we happy with the result? We can plot the __decision boundary__ of our linear regression by imposing that \n",
        "\n",
        "$$\n",
        "x_0 \\cdot \\hat\\theta_0 + \\hat\\theta_1 = 0.5,\n",
        "$$\n",
        "\n",
        "which gives us: \n",
        "\n",
        "$$\n",
        "x_0 = \\frac{(0.5 - \\hat\\theta_1)}{\\hat\\theta_0}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "Rfeuut4QpWvm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Redo plots above\n",
        "plt.plot(x, y_hat)\n",
        "plt.scatter(x, y)\n",
        "plt.ylabel('y')\n",
        "plt.xlabel('X')\n",
        "\n",
        "# Plot the decision boundary\n",
        "decision_boundary = (0.5 - theta_hat[1]) / theta_hat[0]\n",
        "plt.axvline(decision_boundary, c='g');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqhHIRjxOd11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Perceptron\n",
        "\n",
        "The perceptron is a classifier introduced in 1957 by the U.S. Navy. \n",
        "\n",
        "When it was presented, the [NY Times](https://www.nytimes.com/1958/07/08/archives/new-navy-device-learns-by-doing-psychologist-shows-embryo-of.html) [reported](https://sci-hub.tw/10.1177/030631296026003005) that the perceptron was _\"the embryo of an electronic computer that [...] will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"_\n",
        "\n",
        "Today, stacks of perceptrons are used to [walk](https://openai.com/blog/openai-baselines-ppo/), [talk](https://deepmind.com/blog/wavenet-generative-model-raw-audio/),  [see](https://arxiv.org/abs/1409.1556), [write](https://openai.com/blog/better-language-models/), and [reproduce themselves](https://github.com/floodsung/Meta-Learning-Papers), but self-consciousness is still a bit far...\n",
        "\n",
        "--- \n",
        "\n",
        "Conceptually, the perceptron is a simple linear model followed by a Heaviside __activation function__: \n",
        "\n",
        "$$\n",
        "f(X; W) = \\textrm{heaviside}(\\sum\\limits_{i=1}^{m}x_i w_i) =  \\textrm{heaviside}(X^TW)\n",
        "$$\n",
        "\n",
        "with the difference that the training is performed iteratively as: \n",
        "\n",
        "$$\n",
        "W^{(i+1)} = W^{(i)} + \\alpha \\cdot (y - f(X, W^{(i)}) ) \\cdot X\n",
        "$$\n",
        "\n",
        "$\\alpha$ is called the __learning rate__, and it is a way of telling the model by how much to update the parameters $w$.\n",
        "\n",
        "Note that, because of its simple training algorithm, the perceptron is only guaranteed to converge on __linearly separable__ problems. \n",
        "\n",
        "Let's see it in action on a problem with 2D features...\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Dwrk59SKdw2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x, y = make_classification(n_features=2,           # Dimension of the input x\n",
        "                           n_informative=2,        # Number of features that actually have meaning\n",
        "                           n_clusters_per_class=1, # Number of clusters in a class\n",
        "                           n_redundant=0,          # Number of features that carry the same information\n",
        "                           class_sep=3)            # Separation coefficient of the two classes\n",
        "\n",
        "# Let's look at the data\n",
        "c = ['b' if _ == 0 else 'r' for _ in y]  # Tells matplotlib to assign blue to class \"0\" or red to class \"1\"\n",
        "plt.scatter(x[:, 0], x[:, 1], color=c)\n",
        "plt.xlabel('X_0')\n",
        "plt.ylabel('X_1');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqsCwPtRes-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Building a perceptron\n",
        "X = np.hstack((x, np.ones((x.shape[0], 1))))  # Prep the inputs\n",
        "w_init = np.random.normal(0, 1, size=(3,))    # Random weights (with bias)\n",
        "w = w_init\n",
        "\n",
        "# Activation / decision function\n",
        "def heaviside(a):\n",
        "    return 0 if a < 0 else 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hrp7nbIPr6bt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the case of linear regression, we manually set the decision boundary s.t. $X^T \\hat\\theta = 0.5$.\n",
        "\n",
        "For the perceptron, we do something similar, but the boundary is defined by the Heaviside step function\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/325px-Dirac_distribution_CDF.svg.png)\n",
        "\n",
        "so that we we predict the class of a sample as:\n",
        "\n",
        "$$\n",
        "\\hat y = \n",
        "\\begin{cases}\n",
        "0 \\textrm{, if } X^T W < 0 \\\\\n",
        "1 \\textrm{, if } X^T W \\ge 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The decision boundary line, then, is obtained as before, by imposing: \n",
        "\n",
        "$$\n",
        "X^T W = 0.\n",
        "$$\n",
        "\n",
        "In our 2D case, this becomes: \n",
        "\n",
        "$$\n",
        "x_0 \\cdot w_0 +x_1 \\cdot w_1 + w_2 = 0\n",
        "$$\n",
        "\n",
        "which gives us a line: \n",
        "\n",
        "$$\n",
        "x_1 = \\frac{-x_0 \\cdot w_0 - w_2}{w_1} = - \\frac{w_0}{w_1} \\cdot x_0 - \\frac{w_2}{w_1}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Os1F9J_jr4AB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plots\n",
        "plt.scatter(X[:, 0], X[:, 1], color=c)\n",
        "plt.xlabel('X_0')\n",
        "plt.ylabel('X_1')\n",
        "\n",
        "# Plot decision boundary\n",
        "decision_boundary = - (w[0] / w[1]) * X[:, 0] - (w[2] / w[1])\n",
        "plt.plot(X[:, 0], decision_boundary, c='g');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYnfx4LulsXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's train the perceptron\n",
        "learning_rate = 0.001 \n",
        "for _ in range(10):\n",
        "    for s in range(X.shape[0]):\n",
        "        activation = X[s].dot(w)               # Compute activation\n",
        "        prediction = heaviside(activation)     # Make a prediction\n",
        "        error = y[s] - prediction              # Compute the prediction error\n",
        "        update = learning_rate * error * X[s]  # Compute the update\n",
        "        w = w + update                         # Update parameters\n",
        "    print('New w: {}'.format(w))\n",
        "    \n",
        "plt.scatter(X[:, 0], X[:, 1], color=c)\n",
        "plt.xlabel('X_1')\n",
        "plt.ylabel('X_2');\n",
        "\n",
        "# Plot decision boundry\n",
        "decision_boundary = - (w[0] / w[1]) * X[:, 0] - (w[2] / w[1])\n",
        "plt.plot(x[:, 0], decision_boundary, c='g');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w5oSf3_drjEq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import rc\n",
        "from matplotlib import animation\n",
        "rc('animation', html='jshtml')\n",
        "rc('animation', embed_limit=400)\n",
        "\n",
        "# Reset weights\n",
        "w = w_init\n",
        "\n",
        "# Train the perceptron\n",
        "weights = []  # We will save weights here\n",
        "for _ in range(10):\n",
        "    for s in range(X.shape[0]):\n",
        "        activation = X[s].dot(w)\n",
        "        prediction = heaviside(activation)\n",
        "        error = y[s] - prediction\n",
        "        update = learning_rate * error * X[s]\n",
        "        w = w + update\n",
        "        weights.append(w)\n",
        "\n",
        "# Animation\n",
        "fig, ax = plt.subplots()\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], color=c)\n",
        "ax = plt.axis([X[:, 0].min(), X[:, 0].max(), X[:, 1].min(), X[:, 1].max()])\n",
        "\n",
        "# Setup animation\n",
        "w = weights[0]\n",
        "decision_boundary = - (w[0] / w[1]) * X[:, 0] - (w[2] / w[1])\n",
        "line, = plt.plot(X[:, 0], decision_boundary, c='g')\n",
        "\n",
        "def animate(i):\n",
        "    w = weights[i]\n",
        "    decision_boundary = - (w[0] / w[1]) * X[:, 0] - (w[2] / w[1])\n",
        "    line.set_data(X[:, 0], decision_boundary)\n",
        "    return line,\n",
        "\n",
        "# Create animation using the animate() function\n",
        "anim = animation.FuncAnimation(fig, animate, frames=len(weights), interval=10, blit=True, repeat=True)\n",
        "anim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EyFvSXax9QrY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feed-forward neural newtorks\n",
        "\n",
        "\n",
        "Also known as __multi-layer perceptrons__ , neural networks are computational models inspired by the connected structure of the brain. The core component of neural networks is the neuron, which is composed of a perceptron and an activation function: \n",
        "\n",
        "$$\n",
        "f(X; W) = \\sigma ( X^T W).\n",
        "$$\n",
        "\n",
        "The main idea behind neural networks is to compose neurons in two different ways: \n",
        "\n",
        "1. by taking many neurons __in parallel__;\n",
        "2. by composing many subsequent __layers__ of neurons;\n",
        "\n",
        "The result is a network of neurons that take data as input, and compute sequential transformations until the desired result is produced as output.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)\n",
        "\n",
        "---\n",
        "\n",
        "Take the neural network pictured above. Each of the four neurons labeled \"Hidden\", has a very similar form to the perceptron we already saw. \n",
        "\n",
        "In fact, we can write the four neurons as a system: \n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "h_0 = \\sigma(X^T w_0)  = \\sigma(x_0w_{00} + x_1w_{01} + x_2w_{02}) \\\\\n",
        "h_1 = \\sigma(X^T w_1) = \\sigma(x_0w_{10} + x_1w_{11} + x_2w_{12}) \\\\\n",
        "h_2 = \\sigma(X^T w_2) = \\sigma(x_0w_{20} + x_1w_{21} + x_2w_{22}) \\\\\n",
        "h_3 = \\sigma(X^T w_3) =\\sigma( x_0w_{30} + x_1w_{31} + x_2w_{32}) \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "or, better yet, as a vector: \n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \n",
        "h_0 \\\\\n",
        "h_1 \\\\\n",
        "h_2 \\\\\n",
        "h_3 \n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\sigma\\left(\n",
        "\\begin{bmatrix} \n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "x_2\n",
        "\\end{bmatrix}^{\\textrm{ }T}\n",
        "\\begin{bmatrix} \n",
        "w_{00} & w_{10} & w_{20} & w_{30} \\\\\n",
        "w_{01} & w_{11} & w_{21} & w_{31} \\\\\n",
        "w_{02} & w_{12} & w_{22} & w_{32} \\\\\n",
        "\\end{bmatrix}\n",
        "\\right)^{\\textrm{ }T}\n",
        "$$\n",
        "\n",
        "In short, we write the output of a __layer__ of neurons as:\n",
        "$$\n",
        "H = \\sigma(X^TW)^T\n",
        "$$\n",
        "\n",
        "The same is true for the two \"Output\" neurons, with the difference that their input is not $X$, as for the hidden neurons, but it is the output $H$ of the hidden neurons. The output layer can be written as: \n",
        "\n",
        "$$\n",
        "Y = \\sigma(H^TV)^T\n",
        "$$\n",
        "\n",
        "(note that $V$ is a different matrix of parameters).\n",
        "\n",
        "Finally, stacking the two layers simply means __composing__ them together, so that the whole neural network can be written as: \n",
        "\n",
        "$$\n",
        "f(X; W, V) = \\sigma(\\sigma(X^T W) V)^T\n",
        "$$\n",
        "\n",
        "---\n",
        "Neural networks are trained with a technique called __stochastic gradient descent__ (SGD) which is very similar to the perceptron update rule, only generalized to deal with many layers and parallel neurons. The key idea behind SGD is to update all the parameters of the network at the same time, based on how each parameter contributed to the __loss__ function $L(y, f(X; W))$.\n",
        "\n",
        "Here, with $W$ we denote __all__ the parameters on the network, for brevity. \n",
        "\n",
        "The generalized update rule reads: \n",
        "\n",
        "$$\n",
        "W^{(i+1)} = W^{(i)} + \\alpha \\frac{\\partial L(y, f(X, W^{(i)}))}{\\partial w^{(i)}}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is again called __learning rate__.\n",
        "\n",
        "---\n",
        "\n",
        "When training neural networks for binary classification, we take the loss to be the __cross-entropy error function__: \n",
        "\n",
        "$$\n",
        "L(W) =  -\\frac1N \\sum_{n=1}^N \\bigg[y_n  \\log \\hat y_n + (1 - y_n)  \\log (1 - \\hat y_n)\\bigg]\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "jQ3Da3hk7bJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wine quality dataset\n",
        "\n",
        "Let's get started with a basic classification problem. \n",
        "\n",
        "We are given a set of wine reviews, with the following characteristics: "
      ]
    },
    {
      "metadata": {
        "id": "IDM0mA0J7ork",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "data = pd.read_csv(url, delimiter=';')\n",
        "\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yh9yoq9b8bo1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's look at the distribution of the reviews\n",
        "data['quality'].hist();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVkInghm-LIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can turn this into a binary classification problem by setting a threshold on the reviews: was the wine good (>= 6) or not?\n",
        "\n",
        "Notice also how the value of the features are not commensurable with one another. For instance, the \"total sulfur dioxide\" can have values up to 100, while the \"density\" is necessarily limited to be <= 1. \n",
        "\n",
        "While this in principle is not a problem for our machine learning models, in practice it can lead to issues in the training procedure.\n",
        "\n",
        "To standardize the data, we compute the following transformation: \n",
        "\n",
        "$$\n",
        "X_{\\textrm{standardized}} = \\frac{X - \\textrm{mean}(X)}{\\textrm{std}(X)}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "MrUH22Z87x6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "X = data[data.columns[:-1]].values\n",
        "\n",
        "# Normalize features\n",
        "X -= np.mean(X, axis=0)\n",
        "X /= np.std(X, axis=0)\n",
        "\n",
        "# Extact targets\n",
        "quality = data['quality'].values.astype(int)\n",
        "y = quality >= 6\n",
        "plt.hist(y);\n",
        "\n",
        "# Shuffle data\n",
        "p = np.random.permutation(data.shape[0])\n",
        "X = X[p]\n",
        "y = y[p]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dtqk_bGrBFWt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural networks in Python\n",
        "\n",
        "To build our neural network we will use [TensorFlow](https://www.tensorflow.org/), one of the most popular deep learning libraries for Python (the other being [PyTorch](https://pytorch.org/)). \n",
        "TensorFlow provides a huge number of functions, like Numpy, that can be used to manipulate arrays, but offers two great advantages w.r.t. Numpy: \n",
        "\n",
        "1. the computation can be accelerated on GPU via the CUDA library;\n",
        "2. the library implements __automatic differentiation__, meaning that the most analytically complex step of training, the computation of the gradient, is handled for you.\n",
        "\n",
        "While TensorFlow is a very powerful library that offers a fine-coarsened control over what you build, for this course we will skip the low level details and instead use the official high-level API for TensorFlow: [Keras](https://keras.io).\n",
        "\n",
        "# Introduction to TensorFlow/Keras\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/tf_logo_social.png)\n",
        "\n",
        "(most of what follows is adapted from [the introduction on the TensorFlow website](https://www.tensorflow.org/guide/low_level_intro))\n",
        "\n",
        "The most important things to understand about building neural networks with Tensorflow, are the concepts of __tensor__ and __computational graph__. \n",
        "\n",
        "## Tensors\n",
        "A tensor consists of a set of primitive values shaped into an array of any number of dimensions. A tensor's __rank__ is its number of dimensions, while its __shape__ is a tuple of integers specifying the array's length along each dimension. \n",
        "\n",
        "Intuitively, tensors are the TensorFlow version of Numpy arrays. In fact, TF and Numpy are heavily intertwined and arrays can often be fed to TF models without modifications. \n",
        "\n",
        "## Computational graph\n",
        "A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
        "\n",
        "- `tf.Operation` (or \"ops\"): The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
        "- `tf.Tensor`: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors.\n",
        "\n",
        "\n",
        "For this exercise session (and most of the course, probably), you will only need to keep in mind one thing: when using TF, __you first define the computation, and then you provide the data__. \n",
        "\n",
        "This means that all of your operations will be defined on symbolic objects, and only at the end you will actually run the computation.\n",
        "\n",
        "Don't worry if you don't get this at first, it will become clearer by doing. \n",
        "\n",
        "# Keras\n",
        "\n",
        "![alt text](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n",
        "\n",
        "\n",
        "\n",
        "Keras offers collections of TF operations already arranged to implement neural networks with little to no effort. \n",
        "For instance, building a layer of 4 neurons like the one we saw above is as easy as calling `Dense(4)`. That's it. \n",
        "\n",
        "Moreover, Keras offers a high-level API for doing all the usual steps that we usually do when training a neural network, like training on some data, evaluating the performance, and predicting on unseen data. \n",
        "\n",
        "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the `Sequential` model, a linear stack of layers.\n",
        "\n",
        "---\n",
        "\n",
        "In this exercise we will see how to build and train a simple neural network from scratch, in Keras."
      ]
    },
    {
      "metadata": {
        "id": "6jtB0WD-9W9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(32, activation='relu', input_shape=X.shape[1:]))\n",
        "network.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "network.compile(optimizer='sgd', \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['acc'])\n",
        "network.fit(X, y, epochs=100, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cd_1f5SYYU0l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pokémon dataset: unbalanced classes\n",
        "\n",
        "Let's look at a problem where we have a lot of __unbalance__ between the two classes in our data. \n",
        "This will allow us to look at several things: \n",
        "\n",
        "1. How to deal with data normalization\n",
        "2. How to compute __class weights__\n",
        "3. How to evaluate performance on unbalanced problems\n",
        "\n",
        "---\n",
        "\n",
        "Pokémon are fictional creatures that are central to the Pokémon franchise.\n",
        "Among them, Legendary Pokémon are a group of incredibly rare and often very powerful Pokémon, generally featured prominently in the legends and myths of the Pokémon world [[source]](https://bulbapedia.bulbagarden.net/wiki/Legendary_Pok%C3%A9mon).\n",
        "\n",
        "The task that we will tackle in this exercise is simple: can we tell whether a Pokémon is legendary or not, by looking at its stats?\n",
        "\n",
        "Let's start by getting the data and looking at it..."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IFwSoyj7JCd3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/lgreski/pokemonData/master/Pokemon.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7mZIFyeKZUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Like we did before, we will need to standardize the data in order to have commensurable features.\n",
        "\n",
        "However, here we face a problem that we didn't have before: we have substantially less samples of one class w.r.t. the other. This means that that our neural network is likely to ignore samples with $y=1$, because getting right the samples for which $y=0$ will lead to a lower error. \n",
        "\n",
        "Would you study for an exam question that was only asked once by the professor, in previous years? Or would you focus on the more common exercises that are more likely to be asked again? :)\n"
      ]
    },
    {
      "metadata": {
        "id": "vD2yowI0_OW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "X = data[['HP', 'Attack', 'Defense', 'SpecialAtk', 'SpecialDef', 'Speed']].values\n",
        "X = (X - X.mean(0)) / X.std(0)\n",
        "print(X)\n",
        "\n",
        "# Extact targets\n",
        "y = data['Legendary'].values\n",
        "plt.hist(y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDhH9MR_Ktl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To deal with the __class unbalance__ we will use a simple trick, that will allow our model to learn better. \n",
        "\n",
        "The trick consists in __re-weighting__ the loss function, so that the error on rare samples will count more than the error on common samples:\n",
        "\n",
        "$$\n",
        "L_{\\textrm{reweighted}}(y, f(X; W)) =\n",
        "\\begin{cases}\n",
        "\\lambda_0 L(y, f(X; W))\\textrm{, if } y=0 \\\\\n",
        "\\lambda_1 L(y, f(X; W))\\textrm{, if } y=1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Ideally, $\\lambda_0$ and $\\lambda_1$should represent how rare the respective classes are in the dataset. \n",
        "A common way of computing the two values automatically is as: \n",
        "\n",
        "$$\n",
        "\\lambda_i = \\frac{\\textrm{# samples in dataset}}{\\textrm{# classes}\\cdot\\textrm{# samples of class } i}\n",
        "$$\n",
        "\n",
        "In Keras (and also in Scikit-learn) we call these values `class_weight`.\n",
        "\n",
        "Let's see how to compute them..."
      ]
    },
    {
      "metadata": {
        "id": "vLklbJ2wKsku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split train / test / validation data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
        "\n",
        "# Compute class weights\n",
        "n_pokemons = X_train.shape[0]\n",
        "n_legendaries = y_train.sum()\n",
        "class_weights = {0: n_pokemons / (2 * (n_pokemons - n_legendaries)),\n",
        "                 1: n_pokemons / (2 * n_legendaries)}\n",
        "\n",
        "print('Training data: {} legendaries out of {} mons'.format(n_legendaries, n_pokemons))\n",
        "print('Training data: class weights {}'.format(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOMlcPfPNHRd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to train a neural network in Keras using class weights, we only need to apport some minor modifications to the previous model:"
      ]
    },
    {
      "metadata": {
        "id": "I6X4xnhZAfAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(32, activation='relu', input_shape=X.shape[1:]))\n",
        "network.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "network.compile('sgd', 'binary_crossentropy', weighted_metrics=['acc'])\n",
        "\n",
        "network.fit(X, y, epochs=10, validation_data=[X_val, y_val], class_weight=class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V7ANYSbXNSch",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's analyze the __test__ performance of our model:"
      ]
    },
    {
      "metadata": {
        "id": "B4xGKU4tsfWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "    \"\"\"\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.grid(None)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title='Confusion matrix',\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, cm[i, j],\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "eval_results = network.evaluate(X_test, y_test, verbose=False)\n",
        "print('Loss: {:.4f} - Acc: {:.2f}'.format(*eval_results))\n",
        "\n",
        "y_pred = network.predict(X_test)\n",
        "y_pred = np.round(y_pred)\n",
        "plot_confusion_matrix(y_test, y_pred, classes=['normal', 'legendary']); "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}